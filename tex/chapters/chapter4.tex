\cleardoublepage

\chapter{DMD-PM(n) and related Dynamic Mode Decomposition}
\label{chapter:DMD-PM}
\section{Dynamic Mode Decomposition}
Before introducing how to using dynamic mode decomposition to accelerate the power and flattened power method, it may be helpful to review some details of DMD.
As mentioned at Chapter 1, DMD was developed in a variety of fields, therefore, there are are many different varieties, and some of them are combined with other numerical technologies. 
However, most of these varieties share a very familiar straightforward frame, here, I would like to use the most famous and widely-used one, standard DMD, as the example to represent the algorithm. 

To start, first start consider the generic, dynamic problem defined by 
\begin{equation}
  \frac{d{\mathbf{x}}}{dt}=\mathbf{f}(\mathbf{x},t) \, .
  \label{eq:dynamic_problem}
\end{equation}
where $\mathbf{x} \in \mathbb{R}^{n}$ is the $n$-dimensional state vector at time $t$. 
With sufficiently small steps in time, we assume that the evolution of  $\mathbf{x}$ can be well approximated by a relationship of the form 
\begin{equation}
 \frac{d{\mathbf{x}}(t)}{dt}=\mathcal{A}\mathbf{x} \, ,
 \label{eq:linearized_model}
\end{equation}
where the evolution operator $\mathcal{A}$ often may not be known, and can be considered as a implicit black box system.
However, one can obtain the system state $\mathbf{x_n}$ at different times, which are then stacked as the past and future snapshot matrices $ \mathbf{X_0}$ and $ \mathbf{X_1}$ by
\begin{equation}
\label{eq:past_data}
\mathbf{X_0}=\left[ \mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_{m-1} \right] \, ,
\end{equation}
and
\begin{equation}
\label{eq:future_data}
\mathbf{X_1}=\left[ \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_{m} \right] \, .
\end{equation}
where the lower index m is the number of snapshots(observations of the quantities of interest).
Suppose A is the discretized is the discrete-time approximation of the mapping operator $\mathcal{A}$:
\begin{equation}
\label{eq:A}
\mathbf{A} = e^{\mathcal{A} \Delta t} \, .
\end{equation}
Then 
\begin{equation}
\label{eq:x0x1}
\mathbf{x_{k+1}} = \mathbf{A} \mathbf{x_{k}}, \quad \quad k = 0,1,...,
\end{equation}
The approximation operator $\mathbf{A}$ won't fit perfectly unless the samples are actually computed from it, and the most desired condition is to satisfies in a least-squares/minimum-norm sense by
\begin{equation}
\label{eq:least}
\mathbf{A} =  \underset{\mathbf{A}}{argmin} ||\mathbf{X_1} - \mathbf{A} \mathbf{X_0}||_F \, .
\end{equation}
Thus the best-fit operator A is formally given by 
\begin{equation}
\label{eq:fullA}
A = \mathbf{X}_1 \mathbf{X}_0^{\dagger} \, ,
\end{equation}
where $\mathbf{X}_0^{\dagger}$ is the Moore-Penrose generalized inverse of $\mathbf{X}_1$.
It is possible (and typical) to use SVD factorization to inverse $\mathbf{X}_1$ by
\begin{equation}
\label{eq:svd}
\mathbf{X}_0 = \mathbf{U} \bm{\Sigma} \mathbf{V}^{*} \rightarrow \mathbf{X}_0^{\dagger} = \mathbf{V} \bm{\Sigma}^{-1} \mathbf{U}^* \, ,
\end{equation}
where $\mathbf{U} \in \mathbb{C}^{m\times n}$, $\mathbf{V} \in \mathbb{C}^{n\times n}$, $\bm{\Sigma} \in \mathbb{C}^{n\times n}$, and $*$ indicates the conjugate transpose. 

However, consider that number of unknowns is often large in quantity this matrix is large in numerical simulation, the matrix $\mathbf{A}$ is not computed explicitly.
A low-rank approximation of the original dynamic system $\mathbf{\tilde{A}}$ is formed, i.e.,
\begin{equation}
\label{eq:reduced_dmd_1}
\mathbf{\tilde{A}} =  \mathbf{U_r^* A U_r} \, .
\end{equation}
Then using \EQ{eq:fullA}, and the reduced order $\mathbf{\tilde{A}}$ is defined by
\begin{equation}
\label{eq:reduced_dmd_2}
\mathbf{\tilde{A}} =  \mathbf{U_r^*} \mathbf{X_1} \mathbf{V} \bm{\Sigma}^{-1} \, .
\end{equation}
Now extract the r largest eigenvalue and and corresponded eigenvectors from $\mathbf{\tilde{A}}$ as the DMD modes $\boldsymbol{\Phi}$, which can be treated as the leading r eigenvectors of $\mathbf{A}$.
Note that the continue solution of \EQ{eq:linearized_model} is $\mathbf{x}(t) = e^{\mathcal{A}t}\mathbf{x}(0)$, and, $\mathbf{A}$ is discrete-time approximation of $e^{\mathcal{A}\Delta}$, which can be applied to the initial condition by matrix exponential to compute solution at a particular time. 
Moreover, the discrete eigenvalues $\lambda_i$ of $\mathbf{A}$ can be used to compute the continuous eigenvalues $\omega_i= \text{log}(\lambda_i)/\Delta t$, Subsequently, the state can be reconstructed at any time $t$ by initial condition by  
\begin{equation}
\label{eq:dmd_predict}
\vec{x}^{DMD}(t) \triangleq \sum_{i=1}^{r} \vec{\phi}_i e^{\omega_it} b_i \, ,
\end{equation}
where $\mathbf{b}=\boldsymbol{\Phi}^{\dag} \mathbf{x}_{0}$. 
The general DMD scheme is summarized as

\begin{enumerate}
\item Compute SVD decomposition of the forward snapshots matrix, i.e., $ \mathbf{X_0} = \mathbf{U_r} \boldsymbol{\Sigma_r} \mathbf{V_r^{*}}$, where r indicates the rank of matrix.
\item Compute $\mathbf{\tilde{A}}=\mathbf{U_r^{*}X_1}\mathbf{V_r}\boldsymbol{\Sigma}_{\mathbf{r}}^{-1}$, where $\mathbf{A}$ and $\mathbf{\tilde{A}}$ are similar matrices.
\item Compute the eigendecomposition $\mathbf{\tilde{A} \tilde{W}}=\boldsymbol{\Lambda}\mathbf{\tilde{W}}$.
\item Calculate the DMD modes as ${\boldsymbol{\Phi}}={\mathbf{X_2V_r}}\boldsymbol{\Sigma}_\mathbf{r}^{-1}{\mathbf{\tilde{W}}}$.
\item Predict the response by $\vec{x}^{DMD}(t) \approx \sum_{i=1}^{r} \vec{\phi}_i e^{\omega_it} b_i = \boldsymbol{\Phi}{\mathbf{diag}}(e^{\vec{\omega}t})\vec{b}$, where $\mathbf{b}=\boldsymbol{\Phi}^{\dag} \mathbf{x}_{0}$ and $\dagger$ indicates the pseudoinverse.
\end{enumerate}

\section{An Accelerated Power Method using DMD}
\label{sec:dmdpi}
The ultimate goal here is to obtain a strategy that using DMD to extrapolate eigenvectors and eigenvalue from a fair size of snapshot matrix, which should be closed to the final steady state solution than the latest snapshot.
The difficult part of this strategy, due to the prediction results required to be extrapolated in the ``future'', DMD itself needs to extracts infomation from a time dependent dynamic system.
However, the power iteration does not recover the realistic physics transient process. 

Note that $\omega_i= \text{log}(\lambda_i)$ while $\Delta = 1$, $x^{DMD}(t)$ is a fictitious time step corresponding to a single power iteration.
First, suppose that $m$ power iterations have been performed to produce the snapshot matrices $\mathbf{X}_0$ and $\mathbf{X}_1$, where the series of snapshots are not order by the sequence of time, but the number of power iterations.
We follow the standard DMD approach as discussed above and generate leading r DMD modes and eigenvalues.
Next we can set $t$ to a significant value larger than the current number of snapshots employed and predict a new $\mathbf{x}$, which can be normalized and applied as initial guess for a new iteration of power method.

Here, we will explore a modification from original recovery scheme was done in the following way.
Eigendecomposition $\mathbf{A}$ or reduced-order approximation $\mathbf{\tilde{A}}$ leads to a set of approximate eigenvectors for  and a set of eigenvalues $e_j \approx \lambda_j / \lambda_0$.
As previously discussed in \CHAPTER{chapter:PM}, the convergence of the power method require $\lambda_0$ is larger than any other eigenvalues, and, thus, 
\begin{equation}
\label{eq:ej}
e_0 = 1 \quad and  \quad e_j < 1,j = 1,2,3,4... \, .
\end{equation}
There are many strategies to select the optimal rank of DMD modes $r$ used to fit the original system $\mathbf{A}$. 
Alternatively, because the power method can only reveal a single, dominant mode if the assumptions described in \CHAPTER{chapter:PM} are satisfied, then there's only one mode recovered by DMD should remain at $t=\infty$, which is the fundamental eigenmode.
This phenomena can also be proved in numerical perspective, as mentioned at \EQ{eq:ej}, all other modes with eigenvalue smaller than one will vanish eventually while apply the diagonal matrix $e^{\vec{\omega}t}$ for infinity times.
Therefore, instead of computing all the dmd modes and predict the response by \EQ{eq:dmd_predict}, we can simply compute the dominant DMD mode only and predict the system by 
\begin{equation}
\label{eq:f_mode}
\vec{x}^{DMD}(t) \triangleq \vec{\phi}_0 b_0 \, ,
\end{equation}
which can save a great amount of computational resources.
In order to accelerate the power method with DMD, the following DMD-PM($n$) algorithm is proposed:
\begin{enumerate}
 \item Guess $\mathbf{x}^{(0)}$ and normalize.
 \item Perform $n$ power iterations to produce $\mathbf{X}_0$ and $\mathbf{X}_1$
 \item Compute the DMD modes and frequencies using a rank-$r$, truncated  SVD (i.e., $r < n$)
 \item Apply \EQ{eq:dmd_predict} or \EQ{eq:f_mode} to estimate $\mathbf{x}^{(\infty)}=\mathbf{x}(\infty)$, i.e., estimate the steady-state, dominant mode after an equivalent of $\infty$ power iterations.
 \item Set $\mathbf{x}^{(0)} = \Re(\mathbf{x}(\infty)) / ||\mathbf{x}(\infty)||$.  
 \item Repeat Steps 1 through 5 until converged.
\end{enumerate}
Stability analysis between using \EQ{eq:dmd_predict} or \EQ{eq:f_mode} and other numerical challenges are represent in \CHAPTER{chapter:results}.
Nota that the normalization in step 5 is important for reducing numerical round-off errors introduced by growing (or decaying) iterates.  





