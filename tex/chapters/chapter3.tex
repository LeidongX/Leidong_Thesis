% +--------------------------------------------------------------------+
% | Sample Chapter 3
% +--------------------------------------------------------------------+
\cleardoublepage

\chapter{Power Method and Flattened Power Method}

All the eigenvalues and eigenvectors of a single matrix can be calculated by solving characteristic equation.
However, the computational cost can be extremely expensive for a large system. 
For a lot of realistic engineering problem, only the largest eigenvalue and corresponding eigenvector are interesting, which called dominant eigenmodes or dominant eigenpairs.
And the dominant eigenpair satisfies $Ax_0 = \lambda_0 x_0$.
The ratio of neutron gains to losses of neutron transport or diffusion problem shown at last chapter is typical example of chasing dominant eigenmodes.

Power iteration is the simplest method to approximate the fundamental mode and corresponding eigenvalue. 
The iteration method is also known as the Von Mises iteration. 
This method is efficient in solving diagonally dominant sparse matrix which is common in linear systems.
On the other hand, it could access a function evaluating matrix-vector products by $Ax$.
Power iteration has been implemented for a variety kind of fields, including PageRank in their search engine.
Moreover, some of the more advanced eigenvalue algorithms can be understood as variations of the power iteration. 
For instance, the inverse iteration method applies power iteration to the matrix $A^{-1}$ . 
Other algorithms look at the whole subspace generated by the vectors $b_{k}$. 
This subspace is known as the Krylov subspace.
It can be computed by Arnoldi iteration.
\section{Methodology}
\begin{enumerate}
  \item Assume $k_{(0)}$ and $\mathbf{x}_{(0)}$, where $||\mathbf{x}_{(0)}||=1$.
  \item Update $\mathbf{x}_{(i)} = \frac{1}{k_{(i-1)}} \mathbf{A}^{-1} \mathbf{B} \mathbf{x}_{(i-1)}$, where $(i)$ represents the eigen iteration.
  \item Update eigenvalue by  $k_{i} = k_{i-1} \frac{||\mathbf{x}_{i}||}{||\mathbf{x}_{i-1}||}$ and set  $\mathbf{x}_{(i)} =\frac{\mathbf{x}^{}_{i}}{||\mathbf{x}^{}_{i}||}$.
  \item Repeat steps 2 and 3 for $i = 1, 2, \ldots$ until $||\mathbf{x}^{}_{(i)} - \mathbf{x}^{}_{(i-1)}|| < \tau$ for some tolerance $\tau$.
\end{enumerate}

\section{Convergence}

In practice, the usefulness of the power method depends upon the ration $|\lambda_2|/|\lambda_1|$, since it dictates the rate of convergence. 
The danger that q(0) is deficient in x1 (a1 = 0) is a less worrisome matter because if q(0) is chosen randomly the probability for this is 0. 
Moreover, rounding errors sustained during the iteration typicallyensure that the subsequent q(k) have a component in this direction.\begin{verbatim} http://www.cs.huji.ac.il/~csip/tirgul2.pdf\end{verbatim}

organized following by my own language.

As long as the fundamental mode (and eigenvalue) are real and the initial guess $\mathbf{x}^{(0)}$ is not perpendicular to the fundamental mode $\mathbf{x}_0$ (i.e., $\mathbf{x}^T_0\mathbf{x}^{(0)} \neq 0$), the power method will converge to the dominant eigenpair $(\mathbf{x}_0, \lambda_0)$.

The rate of convergence of the power method depends on the relative magnitudes of the leading eigenvalues.  
Let the initial guess be represented as a weighted sum of the eigenvectors of $\mathbf{A}$, i.e.,
\begin{equation}
\begin{split}
  \mathbf{x}^{(0)} 
     &=  c_0' \mathbf{x}_0 + c_1' \mathbf{x}_1 + c_2'  \mathbf{x}_2 \ldots \,  \\
     &=  c_0' \left (\mathbf{x}_0 + \frac{c_1'}{c_0'} \mathbf{x}_1 + \frac{c_2'}{c_0'} \mathbf{x}_2 \ldots  \right ) \\
     &=  c_0' \left (\mathbf{x}_0 + c_1 \mathbf{x}_1 + c_2 \mathbf{x}_2 \ldots  \right )\, .         
\end{split}
\end{equation}
Because normalization of an eigenvector is arbitrary, let $c_0' = 1$.
Then, multiplication of $\mathbf{A}$ by this initial guess leads to
\begin{equation}
\begin{split}
  \mathbf{A} \mathbf{x}^{(0)} 
   &=  \mathbf{A} \mathbf{x}_0  +  c_1 \mathbf{A} \mathbf{x}_1 + c_2 \mathbf{A} \mathbf{x}_2 + \ldots \\
   &= \lambda_0 \mathbf{x}_0 + c_1 \lambda_1 \mathbf{x}_1 + c_2 \lambda_2 \mathbf{x}_2 + \ldots \\ 
   &= \lambda_0 \left ( \mathbf{x}_0 + c_1 \frac{\lambda_1}{\lambda_0} \mathbf{x}_1 + c_2 \frac{\lambda_2}{\lambda_0} \mathbf{x}_2 + \ldots \right ) \, .
\end{split}
\end{equation}
Repeated application of $\mathbf{A}$ leads to
\begin{equation}
\begin{split}
  \mathbf{A}^n \mathbf{x}^{(0)} 
   &= \lambda^n_0 \left ( \mathbf{x}_0 + c_1 \left( \frac{\lambda_1}{\lambda_0} \right)^n \mathbf{x}_1 + c_2 \left ( \frac{\lambda_2}{\lambda_0} \right )^n \mathbf{x}_2 + \ldots \right ) \, ,
\end{split}
\end{equation}
which shows that if $|\lambda_0| > |\lambda_1|$, then $\mathbf{A}^n \mathbf{x}^{(0)}$ will tend toward the direction $\mathbf{x}_0$ at a rate governed by the dominance ratio $|\lambda_1|/|\lambda_0|$. 
Because $\lambda^n_0$ may grow without bound (or vanish), normalization is required during the iteration as is included in the algorithm above.


\section{Flatten Power iteration}
For generalized problems of the form $\mathbf{Ax}= \frac{1}{k} \mathbf{Bx}$, the power method is often used by applying the inverse of $\mathbf{A}$ in order to estimate the largest value of $k$ and its corresponding dominant mode.
For the neutron transport equation, the operator $\mathbf{A}$ requires the inversion of the multigroup transport operator, i.e., its application is equivalent to solving the multigroup equations.
Each iteration, therefore, may require many phase-space sweeps to converge the scattering source.
An alternative often used in practice is the so-called {\it flattened} iteration, in which a single (space-angle-energy) transport sweep is performed for every "power" iteration .
Flattened iterative schemes may require more outer-most iterations (i.e., updates of $k$), but they often reduce the total cost of solving the transport equation (either in eigenvalue  or fixed-source form).
The flattened power method converges the scattering and fission sources simultaneously.
The scattering matrix is moved to the right side of , and, thus, step 2 from the full power iteration becomes
\begin{equation}
 \mathbf{x}_{(i)} =  \mathbf{DL^{-1}M} (\mathbf{S} + \frac{1}{k} \mathbf{F})\mathbf{x}_{(i-1)}   \, .
 \label{eq:flatten}
\end{equation}
The number of transport sweeps (applications of $\mathbf{L}^{-1}$) is a good measure for the total computational cost since one transport sweep is required for each update of the scattering source.